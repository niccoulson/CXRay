{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pGXHVtqxYW8X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGXHVtqxYW8X",
    "outputId": "aa37419b-089c-429a-ab8a-ad2962b595f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# # For Google Colab Only\n",
    "# # Mount GDrive\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "BvwZuVysYaIm",
   "metadata": {
    "id": "BvwZuVysYaIm"
   },
   "outputs": [],
   "source": [
    "# # For Google Colab Only\n",
    "# # Put Colab in the context of this challenge\n",
    "# import os\n",
    "\n",
    "# # os.chdir allows you to change directories, like cd in the Terminal\n",
    "# os.chdir(\"/content/drive/MyDrive/Colab Notebooks/CXRay/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c07d8f8-e079-4e45-9b1a-05c75a56bd25",
   "metadata": {
    "id": "5c07d8f8-e079-4e45-9b1a-05c75a56bd25"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504600a-f546-4258-92e7-61283f3f68f2",
   "metadata": {
    "id": "8504600a-f546-4258-92e7-61283f3f68f2"
   },
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1f0e05-7f0a-423b-8e60-d43afe9d30b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "7d1f0e05-7f0a-423b-8e60-d43afe9d30b1",
    "outputId": "33c81ac6-ba9b-432a-bfad-971396b035de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4ccda731-f9d6-4187-a4e4-78bbf4c7303d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_idx</th>\n",
       "      <th>labels</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>view_pos</th>\n",
       "      <th>label_cardiomegaly</th>\n",
       "      <th>label_effusion</th>\n",
       "      <th>label_no_finding</th>\n",
       "      <th>label_pneumothorax</th>\n",
       "      <th>label_pleural_thickening</th>\n",
       "      <th>label_pneumonia</th>\n",
       "      <th>label_consolidation</th>\n",
       "      <th>label_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ccda731-f9d6-4187-a4e4-78bbf4c7303d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4ccda731-f9d6-4187-a4e4-78bbf4c7303d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4ccda731-f9d6-4187-a4e4-78bbf4c7303d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            img_idx                  labels  age gender view_pos  \\\n",
       "0  00000001_000.png            Cardiomegaly   58      M       PA   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema   58      M       PA   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion   58      M       PA   \n",
       "3  00000002_000.png              No Finding   81      M       PA   \n",
       "4  00000003_000.png                  Hernia   81      F       PA   \n",
       "\n",
       "   label_cardiomegaly  label_effusion  label_no_finding  label_pneumothorax  \\\n",
       "0                True           False             False               False   \n",
       "1                True           False             False               False   \n",
       "2                True            True             False               False   \n",
       "3               False           False              True               False   \n",
       "4               False           False             False               False   \n",
       "\n",
       "   label_pleural_thickening  label_pneumonia  label_consolidation  label_other  \n",
       "0                     False            False                False        False  \n",
       "1                     False            False                False         True  \n",
       "2                     False            False                False        False  \n",
       "3                     False            False                False        False  \n",
       "4                     False            False                False         True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "data_path = pathlib.Path(cwd, \"..\", \"clean_data\", \"cleaned_data.csv\")\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254d3054-b153-4396-938e-3ad847c559c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "254d3054-b153-4396-938e-3ad847c559c7",
    "outputId": "e34954c5-4104-4047-9cde-228da0fd41f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112104, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.drop(columns=\"label_other\")\n",
    "data = data.drop(columns=\"label_no_finding\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b869784-c376-4e9b-a6f7-d52d59463f8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b869784-c376-4e9b-a6f7-d52d59463f8a",
    "outputId": "5a64b35d-ed15-40d9-afd3-a95db60f373e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_cardiomegaly',\n",
       " 'label_effusion',\n",
       " 'label_pneumothorax',\n",
       " 'label_pleural_thickening',\n",
       " 'label_pneumonia',\n",
       " 'label_consolidation',\n",
       " 'label_other']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [l for l in data.columns if l.startswith(\"label_\")]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf34114-2cd5-4e38-905c-537dafed3d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cf34114-2cd5-4e38-905c-537dafed3d25",
    "outputId": "60b0cad0-0ae0-4b84-b133-a15dd64a57fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_pneumonia              1430\n",
       "label_cardiomegaly           2776\n",
       "label_pleural_thickening     3384\n",
       "label_consolidation          4667\n",
       "label_pneumothorax           5301\n",
       "label_effusion              13316\n",
       "label_other                 39770\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts_sorted = data[label_names].sum(axis=0).sort_values()\n",
    "label_counts_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae949f0e-5b60-4f62-82cc-7d31ce90d706",
   "metadata": {
    "id": "ae949f0e-5b60-4f62-82cc-7d31ce90d706"
   },
   "source": [
    "We want to have a balanced dataset that contains an equal number of examples of each condition (+\"no finding\"), regardless of 'overlaps' (i.e. multiple conditions).  \n",
    "\n",
    "We will start from the lowest common denominator, i.e. the condition with the fewest number of examples, i.e. pnemonia. We will choose **all** of the pneumonia cases, `N0 = N_pneumonia = 1430` above. Then we have to choose the same number of examples from each of the other conditions, in increasing order of the number of cases. But there are overlaps, so we will exclude cases already chosen from previous labels (i.e. those with fewer examples). We will keep adding examples until we have `N0` rows with each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4300583-52ac-44a7-8651-f7030d22a219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4300583-52ac-44a7-8651-f7030d22a219",
    "outputId": "c9c0283a-d270-4fa5-f393-a0b86a107d1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_pneumothorax          1430\n",
       "label_pneumonia             1430\n",
       "label_consolidation         1431\n",
       "label_cardiomegaly          1439\n",
       "label_pleural_thickening    1522\n",
       "label_effusion              1585\n",
       "label_other                 3287\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rarest_label = label_counts_sorted.index[0]\n",
    "rarest_label_count = data[data[rarest_label]].shape[0]\n",
    "data_balanced = data[data[rarest_label]]\n",
    "n_min = data_balanced.shape[0]\n",
    "for label, count in zip(label_counts_sorted.index[1:], label_counts_sorted[1:]):\n",
    "    n_already_captured = data_balanced[data_balanced[label]].shape[0]\n",
    "    n_additional = n_min - n_already_captured\n",
    "    if n_additional > 0:\n",
    "        not_selected_indices = data.index.difference(data_balanced.index)\n",
    "        not_selected_data = data.loc[not_selected_indices]\n",
    "        not_selected_data_pa = not_selected_data.query(\"view_pos == 'PA'\").copy()\n",
    "        not_selected_data_ap = not_selected_data.query(\"view_pos == 'AP'\").copy()\n",
    "        rows_to_add = not_selected_data_pa[not_selected_data_pa[label]].sample(n_additional)\n",
    "        data_balanced = pd.concat([data_balanced, rows_to_add], axis=0)\n",
    "\n",
    "data_balanced[label_names].sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30e3c5-ea59-4062-8db4-9020aa21ecc1",
   "metadata": {
    "id": "5e30e3c5-ea59-4062-8db4-9020aa21ecc1"
   },
   "source": [
    "Actually let's keep a small percentage of the rarest class (pneumonia) for testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1e2d43-990c-47e5-8e0d-597642bdb37f",
   "metadata": {
    "id": "0c1e2d43-990c-47e5-8e0d-597642bdb37f"
   },
   "outputs": [],
   "source": [
    "rarest_label_fraction_to_keep = 0.1\n",
    "indices_to_remove = (\n",
    "    data_balanced[data_balanced[rarest_label]]\n",
    "    .sample(int(rarest_label_count * rarest_label_fraction_to_keep))\n",
    "    .index\n",
    ")\n",
    "data_balanced.drop(indices_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c63b40-b694-4643-96dc-bdeccdda5b86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7c63b40-b694-4643-96dc-bdeccdda5b86",
    "outputId": "0f506f32-a77d-4faa-a620-4dc5fb3b5075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6345, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e9ab967-8e37-484b-93cc-ddc3ed6429fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "1e9ab967-8e37-484b-93cc-ddc3ed6429fb",
    "outputId": "51fcbaa9-ee39-416d-9746-191f0da9b454"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_balanced[label_names].sum(axis=0).sort_values().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a13e7-7211-4cec-9e80-edbba78afcca",
   "metadata": {
    "id": "597a13e7-7211-4cec-9e80-edbba78afcca"
   },
   "source": [
    "We can use the remaining (i.e. not selected) data for our test set later on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11123ac4-4c52-4ad9-8f8b-f54d2cd91990",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "11123ac4-4c52-4ad9-8f8b-f54d2cd91990",
    "outputId": "08ffe7cc-bcd3-4f99-e591-b0759f1f6229"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "remaining_indices = data.index.difference(data_balanced.index)\n",
    "remaining_data = data.loc[remaining_indices]\n",
    "remaining_data[label_names].sum(axis=0).sort_values().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20ee2ec-3ede-4e0c-a71a-7a845f5007a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "a20ee2ec-3ede-4e0c-a71a-7a845f5007a7",
    "outputId": "bd532f27-881d-4a12-e53c-2ffe794bec35"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_balanced.view_pos.value_counts().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2580dc2-80cc-4f47-9e28-37fb905bb4a1",
   "metadata": {
    "id": "f2580dc2-80cc-4f47-9e28-37fb905bb4a1"
   },
   "source": [
    "# Loading the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec4150-96a6-4cc6-bf4a-2770994fb612",
   "metadata": {
    "id": "6cec4150-96a6-4cc6-bf4a-2770994fb612"
   },
   "source": [
    "There are two ways to go about this:\n",
    "1. Load the entire data set in memory if we have enough RAM, which we do. We estimate ~8000 images will take up between 4 to 8 gigabytes in RAM.\n",
    "2. \"Dataset generator\": Keras has an \"[image data generator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99be35-a48c-4e9a-bb4e-2ae934c83b89",
   "metadata": {
    "id": "3b99be35-a48c-4e9a-bb4e-2ae934c83b89"
   },
   "source": [
    "## Loading the Entire Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b2875-b0ad-44cb-b94a-05593a5f6422",
   "metadata": {
    "id": "680b2875-b0ad-44cb-b94a-05593a5f6422"
   },
   "source": [
    "Let's try the first method first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9a73848-7296-42e1-9e99-945bd2ddf375",
   "metadata": {
    "id": "c9a73848-7296-42e1-9e99-945bd2ddf375"
   },
   "outputs": [],
   "source": [
    "# def load_img(img_filename):\n",
    "#     data_dir = pathlib.Path(\"../raw_data/images_224_224\")\n",
    "#     image_filename = img_filename\n",
    "#     image = PIL.Image.open(pathlib.Path(data_dir, image_filename))\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ec4e96e-ea3d-483d-aa3b-cf784cf9da0d",
   "metadata": {
    "id": "1ec4e96e-ea3d-483d-aa3b-cf784cf9da0d"
   },
   "outputs": [],
   "source": [
    "# n_images = data_balanced.shape[0]\n",
    "# example_img = load_img(data_balanced.iloc[0, :][\"img_idx\"])\n",
    "# img_dims = example_img.size\n",
    "# img_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68945115-6f3f-46f5-96b6-231e704d9f3d",
   "metadata": {
    "id": "68945115-6f3f-46f5-96b6-231e704d9f3d"
   },
   "outputs": [],
   "source": [
    "# img_data = np.zeros((n_images, *img_dims))\n",
    "# img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f16098ef-2a27-4eeb-9e86-d909e5158f3f",
   "metadata": {
    "id": "f16098ef-2a27-4eeb-9e86-d909e5158f3f"
   },
   "outputs": [],
   "source": [
    "# # This crashes Jupyter Notebook\n",
    "# for img in data_balanced.loc[:, \"img_idx\"]:\n",
    "#     # np.append(img_data, load_img(img))\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932daf17-a678-49c0-a263-4f356148deb7",
   "metadata": {
    "id": "932daf17-a678-49c0-a263-4f356148deb7"
   },
   "source": [
    "That was a dud. It crashes Jupyter Notebook. Let us now explore the second method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69babdb7-55d8-4e5d-9227-c5bb74bdd3ff",
   "metadata": {
    "id": "69babdb7-55d8-4e5d-9227-c5bb74bdd3ff"
   },
   "source": [
    "## Generating the Dataset on the Fly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e184236-774c-4fbd-b8e0-af301d53a897",
   "metadata": {
    "id": "5e184236-774c-4fbd-b8e0-af301d53a897"
   },
   "source": [
    "It turns out [`tf.keras.preprocessing.image.ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) is deprecated and hence not recommended for new code. Neither it nor its successor, [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) are really applicable to our multi-label problem. (They are suited to single-label classification problems). I found [this blog post](https://towardsdatascience.com/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72) on how to leverage the underlying API, [`tf.data`](https://www.tensorflow.org/guide/data) for multi-label problems. Let us explore that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f73f540-5c71-4830-b4f9-d6a1000af377",
   "metadata": {
    "id": "2f73f540-5c71-4830-b4f9-d6a1000af377"
   },
   "outputs": [],
   "source": [
    "# In order to do 'transfer learning' using VGG16, ResNet, etc. our image needs to have three channels.\n",
    "# We will simply duplicate the one grayscale channel 2 more times.\n",
    "def expand_greyscale_image_channels(image):\n",
    "    if image.shape[-1] == 1:\n",
    "        grey_image_3_channel = tf.tile(image, tf.constant([1, 1, 3], tf.int32))\n",
    "    else:\n",
    "        grey_image_3_channel = image\n",
    "\n",
    "    return tf.keras.applications.vgg16.preprocess_input(grey_image_3_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fcbfe1b-02be-4c54-859f-ffb5fc700d66",
   "metadata": {
    "id": "7fcbfe1b-02be-4c54-859f-ffb5fc700d66"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224  # img_dims[0]\n",
    "CHANNELS = 3  # The images WERE grayscale but we converted them to 3-channel\n",
    "\n",
    "\n",
    "def parse_function(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    filepath = \"gs://cxray-bucket/images_224_224/\" + filename\n",
    "    # Read an image from a file\n",
    "    # image_string = tf.io.read_file(filename)\n",
    "    image_string = tf.io.read_file(filepath)\n",
    "    # with tf.io.gfile.GFile(f\"gs://cxray-bucket/images_224_224/{filename}\", \"rb\") as f:\n",
    "    #     image_string = f.read()\n",
    "    # image_string = tf.io.gfile.GFile(f\"gs://cxray-bucket/images_224_224/{filename}\", \"rb\").read()\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=CHANNELS)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = (\n",
    "        image_decoded  # tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    )\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = tf.cast(image_resized, tf.float32) # / 255.0 (the scaling is taken care of by `preprocess_input()` above)\n",
    "    image_normalized_rgb = expand_greyscale_image_channels(image_normalized)\n",
    "    label_tensor = tf.convert_to_tensor(label, dtype=tf.uint8)\n",
    "    return image_normalized_rgb, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52d232b7-4da7-4f28-bc46-d0bd28699c28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52d232b7-4da7-4f28-bc46-d0bd28699c28",
    "outputId": "06cc0f23-51bb-4d97-8506-fd6f0f322119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([224, 224, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cwd = os.getcwd()\n",
    "# data_dir = pathlib.Path(cwd, \"..\", \"raw_data\", \"images_224_224\")\n",
    "\n",
    "# This works\n",
    "img_norm_rgb, label_tensor = parse_function(\n",
    "    \"00000009_000.png\", # str(pathlib.Path(data_dir, \"00000009_000.png\")),\n",
    "    np.array([True, False, False, True]) + 0,\n",
    ")\n",
    "img_norm_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72789baf-a72d-4f39-b9f3-3b28a84b5dfa",
   "metadata": {
    "id": "72789baf-a72d-4f39-b9f3-3b28a84b5dfa"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 256  # The blog post suggested 256 is big enough to measure an F1-score but I suspect this was leading to this error while I was training the model: BiasGrad requires tensor size <= int32 max\n",
    "AUTOTUNE = (\n",
    "    tf.data.experimental.AUTOTUNE\n",
    ")  # Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\n",
    "SHUFFLE_BUFFER_SIZE = (\n",
    "    256 # 1024  # Shuffle the training data by a chunck of 1024 observations\n",
    ")\n",
    "\n",
    "N_LABELS = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd029e70-ca81-47dc-9ea4-14ac9ffa93b6",
   "metadata": {
    "id": "cd029e70-ca81-47dc-9ea4-14ac9ffa93b6"
   },
   "outputs": [],
   "source": [
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if is_training == True:\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        dataset = dataset.cache()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48448164-e18a-4908-b810-a4cc53243565",
   "metadata": {
    "id": "48448164-e18a-4908-b810-a4cc53243565"
   },
   "outputs": [],
   "source": [
    "filenames = data_balanced.img_idx.to_list()\n",
    "filepaths = filenames # [str(pathlib.Path(data_dir, filename)) for filename in filenames]\n",
    "label_sets_binarized = np.array(\n",
    "    data_balanced[label_names].apply(lambda x: x + 0), dtype=np.uint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "578427ce-6145-4fc5-88ce-48a910850f84",
   "metadata": {
    "id": "578427ce-6145-4fc5-88ce-48a910850f84"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train_bin, y_val_bin = train_test_split(\n",
    "    filepaths, label_sets_binarized, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f321de-c8e3-4561-853e-ed558b2cae3d",
   "metadata": {
    "id": "07f321de-c8e3-4561-853e-ed558b2cae3d"
   },
   "outputs": [],
   "source": [
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07afce-e819-4c6b-be05-3f5579954531",
   "metadata": {
    "id": "dc07afce-e819-4c6b-be05-3f5579954531"
   },
   "source": [
    "We now, finally, have our training and validation data sets with the correct tensor sizes and data types.  \n",
    "Now let's create the test set from the data that was _not_ selected while creating the balanced dataset. (Stored in `remaining_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0Dz8ktSe-dFn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Dz8ktSe-dFn",
    "outputId": "161f7d4b-3af6-4dcf-9b30-fbc130f9b3ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_pneumothorax          143\n",
       "label_pneumonia             143\n",
       "label_consolidation         145\n",
       "label_cardiomegaly          150\n",
       "label_pleural_thickening    150\n",
       "label_effusion              172\n",
       "label_other                 356\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a balanced subset...\n",
    "test_data_balanced = remaining_data[remaining_data[rarest_label]]\n",
    "n_min = test_data_balanced.shape[0]\n",
    "\n",
    "for label, count in zip(label_counts_sorted.index[1:], label_counts_sorted[1:]):\n",
    "    n_already_captured = test_data_balanced[test_data_balanced[label]].shape[0]\n",
    "    n_additional = n_min - n_already_captured\n",
    "    if n_additional > 0:\n",
    "        not_selected_indices = remaining_data.index.difference(test_data_balanced.index)\n",
    "        not_selected_data = remaining_data.loc[not_selected_indices]\n",
    "        rows_to_add = not_selected_data[not_selected_data[label]].sample(n_additional)\n",
    "        test_data_balanced = pd.concat([test_data_balanced, rows_to_add], axis=0)\n",
    "\n",
    "test_data_balanced[label_names].sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8416c797-34f4-49b0-a869-8ba9c6eaa80c",
   "metadata": {
    "id": "8416c797-34f4-49b0-a869-8ba9c6eaa80c"
   },
   "outputs": [],
   "source": [
    "filenames_test = test_data_balanced.img_idx.to_list()\n",
    "X_test = filenames_test # [str(pathlib.Path(data_dir, filename)) for filename in filenames_test]\n",
    "y_test_bin = np.array(\n",
    "    test_data_balanced[label_names].apply(lambda x: x + 0), dtype=np.uint8\n",
    ")\n",
    "test_ds = create_dataset(X_test, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac05cae-cfc0-4306-b611-1fab5552e390",
   "metadata": {
    "id": "1ac05cae-cfc0-4306-b611-1fab5552e390"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b49c66-669b-4057-899b-3290537cdeee",
   "metadata": {
    "id": "05b49c66-669b-4057-899b-3290537cdeee"
   },
   "source": [
    "## Loss Function and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72d6ae-de4d-477a-adab-c9e2010af3a1",
   "metadata": {
    "id": "7f72d6ae-de4d-477a-adab-c9e2010af3a1"
   },
   "source": [
    "The function below, from [the Medium post mentioned earlier](https://towardsdatascience.com/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72), will provide our evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c3a2129-0689-4629-9138-ee2243b8e26f",
   "metadata": {
    "id": "2c3a2129-0689-4629-9138-ee2243b8e26f"
   },
   "outputs": [],
   "source": [
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "    \"\"\"Compute the macro F1-score on a batch of observations (average F1 across labels)\n",
    "\n",
    "    Args:\n",
    "        y (int32 Tensor): labels array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        thresh: probability value above which we predict positive\n",
    "\n",
    "    Returns:\n",
    "        macro_f1 (scalar Tensor): value of macro F1 for the batch\n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2 * tp / (2 * tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552cd2b-c1f2-4de6-a113-a224aa7b7bfc",
   "metadata": {
    "id": "5552cd2b-c1f2-4de6-a113-a224aa7b7bfc"
   },
   "source": [
    "And the one below, from a related blog post ([here](https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)), provides a differentiable version of the above, for our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7bfd91f-e5d4-4911-8ff9-5ec9fc05bdae",
   "metadata": {
    "id": "d7bfd91f-e5d4-4911-8ff9-5ec9fc05bdae"
   },
   "outputs": [],
   "source": [
    "def macro_soft_f1(y, y_hat):\n",
    "    \"\"\"Compute the macro soft F1-score as a cost.\n",
    "    Average (1 - soft-F1) across all labels.\n",
    "    Use probability values instead of binary predictions.\n",
    "\n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix of shape (BATCH_SIZE, N_LABELS)\n",
    "\n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2 * tp / (2 * tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1  # reduce 1 - soft-f1 in order to increase soft-f1\n",
    "    macro_cost = tf.reduce_mean(cost)  # average on all labels\n",
    "\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a5ec4-72cf-4ffd-9c16-288e6a3195cf",
   "metadata": {
    "id": "397a5ec4-72cf-4ffd-9c16-288e6a3195cf"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5978b-c44a-4879-a5df-b451bb85f3d0",
   "metadata": {
    "id": "95a5978b-c44a-4879-a5df-b451bb85f3d0"
   },
   "source": [
    "What if we just returned ones across the board? (i.e. naively predict that all patients have all of the conditions)  \n",
    "Or if we randomly predicted the conditions based on their frequencies of occurrence?  \n",
    "We need _something_ to compare our eventual model's evaluation metrics to.  \n",
    "Let us explore this further now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95e210b8-b30f-4828-b87c-17c2d65ee861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95e210b8-b30f-4828-b87c-17c2d65ee861",
    "outputId": "730373d0-55bf-4657-c5f4-34d274e41a4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.42218798>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_all_ones = np.ones(y_test_bin.shape)\n",
    "macro_f1(y_test_bin, pred_all_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54fc57b8-7ea9-4980-8fb8-172f47127b66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54fc57b8-7ea9-4980-8fb8-172f47127b66",
    "outputId": "3a56a4c0-e018-4898-dfd6-4ced502bdaa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_cardiomegaly          0.226478\n",
       "label_effusion              0.245548\n",
       "label_pneumothorax          0.224429\n",
       "label_pleural_thickening    0.238455\n",
       "label_pneumonia             0.202837\n",
       "label_consolidation         0.224113\n",
       "label_other                 0.503861\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ratios = data_balanced[label_names].sum(axis=0) / len(data_balanced)\n",
    "label_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a81f9e1-5a84-459f-a854-24aff07a84a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a81f9e1-5a84-459f-a854-24aff07a84a9",
    "outputId": "aaaaaa33-b9bb-4194-e4d5-11d3a1a3655f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_cardiomegaly          148.0\n",
       "label_effusion              160.0\n",
       "label_pneumothorax          146.0\n",
       "label_pleural_thickening    155.0\n",
       "label_pneumonia             132.0\n",
       "label_consolidation         146.0\n",
       "label_other                 329.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_random_label_counts = round(y_test_bin.shape[0] * label_ratios)\n",
    "pred_random_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f47305e-9373-41c1-859c-6f8838939e2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f47305e-9373-41c1-859c-6f8838939e2d",
    "outputId": "f4e4a772-e724-4aa6-9eb3-a60ede0204bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.25882468>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_random = np.zeros(y_test_bin.shape)\n",
    "\n",
    "for i, label in enumerate(label_names):\n",
    "    pred_random[np.random.choice(pred_random.shape[0], int(pred_random_label_counts[label]), replace=False), i] = 1\n",
    "\n",
    "# pred_random.sum(axis=0)\n",
    "macro_f1(y_test_bin, pred_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a142a8c-79a8-47a7-82cf-79f2b40bff46",
   "metadata": {
    "id": "4a142a8c-79a8-47a7-82cf-79f2b40bff46"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81470564-3cf6-4512-bf1c-2fc8ea91d4de",
   "metadata": {
    "id": "81470564-3cf6-4512-bf1c-2fc8ea91d4de"
   },
   "source": [
    "We will be performing transfer learning using one of the many available pre-trained deep convolutional neural networks such as those listed in [this paper](https://www.nature.com/articles/s41598-020-70479-z.pdf?origin=ppubhttps://www.nature.com/articles/s41598-020-70479-z.pdf?origin=ppub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ecc0b5d-7fbc-4324-9877-129784cdbf73",
   "metadata": {
    "id": "6ecc0b5d-7fbc-4324-9877-129784cdbf73"
   },
   "outputs": [],
   "source": [
    "LR = 1e-5  # For transfer learning, use a small value such as 1e5\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def get_pretrained_model():\n",
    "    pretrained_model = tf.keras.applications.vgg16.VGG16( # resnet50.ResNet50(\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, CHANNELS),\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65d745b5-138b-4a2a-ab5c-067be8a89e38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65d745b5-138b-4a2a-ab5c-067be8a89e38",
    "outputId": "9b71a2e0-8558-4c1e-938b-873b74bdb326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pretrained = get_pretrained_model()\n",
    "# pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ffa302-d03e-4f9d-b6b2-6976b358c2ec",
   "metadata": {
    "id": "40ffa302-d03e-4f9d-b6b2-6976b358c2ec"
   },
   "outputs": [],
   "source": [
    "def create_compiled_custom_model():\n",
    "    base_model = get_pretrained_model()\n",
    "    flattening_layer = tf.keras.layers.Flatten()\n",
    "    dense_layer_1 = tf.keras.layers.Dense(250, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "    dropout_layer_1 = tf.keras.layers.Dropout(0.4)\n",
    "    dense_layer_2 = tf.keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "    dropout_layer_2 = tf.keras.layers.Dropout(0.2)\n",
    "    dense_layer_3 = tf.keras.layers.Dense(50, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "    dropout_layer_3 = tf.keras.layers.Dropout(0.1)\n",
    "    prediction_layer = tf.keras.layers.Dense(N_LABELS, activation=\"sigmoid\")\n",
    "\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            flattening_layer,\n",
    "            dense_layer_1,\n",
    "            dropout_layer_1,\n",
    "            dense_layer_2,\n",
    "            dropout_layer_2,\n",
    "            dense_layer_3,\n",
    "            dropout_layer_3,\n",
    "            prediction_layer,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=macro_soft_f1,\n",
    "        metrics=[macro_f1],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe33c255-b992-4ef9-a3b0-6afcb8270d2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe33c255-b992-4ef9-a3b0-6afcb8270d2d",
    "outputId": "46e33eac-1aac-4f5f-9bb0-3e265e4faa71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               6272250   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 250)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               25100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 357       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,017,445\n",
      "Trainable params: 6,302,757\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model = create_compiled_custom_model()\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97cfc223-e395-4018-a44f-5055982e471e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "97cfc223-e395-4018-a44f-5055982e471e",
    "outputId": "34f3bbc7-9b14-492b-9097-f432aab84665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 19/159 [==>...........................] - ETA: 49:39 - loss: 0.7750 - macro_f1: 0.2850"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-540a1a8b897e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0;31m history = test_model.fit(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"model.{epoch:03d}.h5\",\n",
    "    monitor=\"val_macro_f1\",\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    ")\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor=\"val_macro_f1\", mode=\"max\", patience=5, verbose=1, restore_best_weights=True\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "history = test_model.fit(\n",
    "    train_ds, epochs=EPOCHS, validation_data=val_ds, verbose=1, callbacks=[es, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UxftrAHT9Syd",
   "metadata": {
    "id": "UxftrAHT9Syd"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    # --- LOSS: Soft Macro F1 ---\n",
    "    ax[0].plot(history.history[\"loss\"])\n",
    "    ax[0].plot(history.history[\"val_loss\"])\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_ylabel(\"Soft Macro F1\")\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].legend([\"Train\", \"Validation\"], loc=\"best\")\n",
    "    ax[0].grid(axis=\"x\", linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\", linewidth=0.5)\n",
    "\n",
    "    # --- METRICS: Macro F1 ---\n",
    "\n",
    "    ax[1].plot(history.history[\"macro_f1\"])\n",
    "    ax[1].plot(history.history[\"val_macro_f1\"])\n",
    "    ax[1].set_title(\"Metric\")\n",
    "    ax[1].set_ylabel(\"Macro F1\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].legend([\"Train\", \"Validation\"], loc=\"best\")\n",
    "    ax[1].grid(axis=\"x\", linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\", linewidth=0.5)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7FLpbinD9U3R",
   "metadata": {
    "id": "7FLpbinD9U3R"
   },
   "outputs": [],
   "source": [
    "plot_history(history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c0d4c-4e1f-40de-b1ff-1b40033fe09b",
   "metadata": {
    "id": "ed1c0d4c-4e1f-40de-b1ff-1b40033fe09b"
   },
   "outputs": [],
   "source": [
    "res = test_model.evaluate(test_ds, return_dict=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GTV2FtdDN07H",
   "metadata": {
    "id": "GTV2FtdDN07H"
   },
   "outputs": [],
   "source": [
    "filename = f\"test_model_vgg16_bs{BATCH_SIZE}_do40-20-10_l2-0001(Colab).pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neMXeKSReh3D",
   "metadata": {
    "id": "neMXeKSReh3D"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "test_model.save(filename)\n",
    "# pickle.dump(test_model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VCKq9M4_efMP",
   "metadata": {
    "id": "VCKq9M4_efMP"
   },
   "outputs": [],
   "source": [
    "# loaded_model = pickle.load(open(filename, \"rb\"))\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    filename,\n",
    "    custom_objects={\"macro_f1\": macro_f1, \"macro_soft_f1\": macro_soft_f1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pYgzhvD2QDpM",
   "metadata": {
    "id": "pYgzhvD2QDpM"
   },
   "outputs": [],
   "source": [
    "y_pred = []  # store predicted labels\n",
    "y_true = []  # store true labels\n",
    "\n",
    "# iterate over the dataset\n",
    "for image_batch, label_batch in test_ds:   # use dataset.unbatch() with repeat\n",
    "   # append true labels\n",
    "   y_true.append(label_batch)\n",
    "   # compute predictions\n",
    "   preds = test_model.predict(image_batch)\n",
    "   # append predicted labels\n",
    "#    y_pred.append(preds)\n",
    "#    y_pred.append((preds > 0.5))\n",
    "   y_pred.append((preds > 0.5) * 1)\n",
    "\n",
    "# convert the true and predicted labels into tensors\n",
    "correct_labels = tf.concat([item for item in y_true], axis = 0)\n",
    "predicted_labels = tf.concat([item for item in y_pred], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ham8KN2m2fak",
   "metadata": {
    "id": "Ham8KN2m2fak"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels_array = np.array(label_names)\n",
    "y_true = correct_labels\n",
    "y_pred = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UtjHhie2lp0l",
   "metadata": {
    "id": "UtjHhie2lp0l"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=label_names))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
